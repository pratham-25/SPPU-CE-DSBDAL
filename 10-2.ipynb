{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9516f56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73b88f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample.txt') as file:\n",
    "    para = file.read()\n",
    "    para = para.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "218e60ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artificial intelligence, or ai, has become a significant field of study and research in recent years. with its ability to simulate human intelligence and perform complex tasks, ai has found applications in various domains such as healthcare, finance, and technology. the advancements in machine learning and deep learning algorithms have enabled ai systems to analyze vast amounts of data and make informed decisions. one of the key areas where ai has made significant strides is natural language processing, or nlp. nlp focuses on enabling computers to understand and interpret human language. it involves tasks such as text classification, sentiment analysis, and language generation. nlp has revolutionized the way we interact with technology, powering voice assistants and chatbots that can understand and respond to human queries. in addition to nlp, ai has also made significant contributions to computer vision. computer vision algorithms can analyze and interpret visual data, enabling machines to recognize objects, faces, and even emotions. this technology has found applications in autonomous vehicles, surveillance systems, and medical imaging. the ethical implications of ai have also garnered attention. issues such as privacy, bias, and accountability need to be addressed to ensure responsible ai development and deployment. organizations and policymakers are working towards establishing guidelines and frameworks to govern ai systems and their impact on society. as ai continues to evolve, the possibilities are endless. from personalized recommendation systems to predictive analytics, ai has the potential to transform industries and enhance human capabilities. however, it is crucial to strike a balance between technological advancements and ethical considerations to harness the full potential of ai for the benefit of humanity.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(para)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a636efa4",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ce36a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "tokenized_sent = sent_tokenize(para)\n",
    "tokenized_word = word_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8c91ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence:  16 ['artificial intelligence, or ai, has become a significant field of study and research in recent years.', 'with its ability to simulate human intelligence and perform complex tasks, ai has found applications in various domains such as healthcare, finance, and technology.', 'the advancements in machine learning and deep learning algorithms have enabled ai systems to analyze vast amounts of data and make informed decisions.', 'one of the key areas where ai has made significant strides is natural language processing, or nlp.', 'nlp focuses on enabling computers to understand and interpret human language.', 'it involves tasks such as text classification, sentiment analysis, and language generation.', 'nlp has revolutionized the way we interact with technology, powering voice assistants and chatbots that can understand and respond to human queries.', 'in addition to nlp, ai has also made significant contributions to computer vision.', 'computer vision algorithms can analyze and interpret visual data, enabling machines to recognize objects, faces, and even emotions.', 'this technology has found applications in autonomous vehicles, surveillance systems, and medical imaging.', 'the ethical implications of ai have also garnered attention.', 'issues such as privacy, bias, and accountability need to be addressed to ensure responsible ai development and deployment.', 'organizations and policymakers are working towards establishing guidelines and frameworks to govern ai systems and their impact on society.', 'as ai continues to evolve, the possibilities are endless.', 'from personalized recommendation systems to predictive analytics, ai has the potential to transform industries and enhance human capabilities.', 'however, it is crucial to strike a balance between technological advancements and ethical considerations to harness the full potential of ai for the benefit of humanity.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenized Sentence: \",len(tokenized_sent), tokenized_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e9eb5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Words:  304 ['artificial', 'intelligence', ',', 'or', 'ai', ',', 'has', 'become', 'a', 'significant', 'field', 'of', 'study', 'and', 'research', 'in', 'recent', 'years', '.', 'with', 'its', 'ability', 'to', 'simulate', 'human', 'intelligence', 'and', 'perform', 'complex', 'tasks', ',', 'ai', 'has', 'found', 'applications', 'in', 'various', 'domains', 'such', 'as', 'healthcare', ',', 'finance', ',', 'and', 'technology', '.', 'the', 'advancements', 'in', 'machine', 'learning', 'and', 'deep', 'learning', 'algorithms', 'have', 'enabled', 'ai', 'systems', 'to', 'analyze', 'vast', 'amounts', 'of', 'data', 'and', 'make', 'informed', 'decisions', '.', 'one', 'of', 'the', 'key', 'areas', 'where', 'ai', 'has', 'made', 'significant', 'strides', 'is', 'natural', 'language', 'processing', ',', 'or', 'nlp', '.', 'nlp', 'focuses', 'on', 'enabling', 'computers', 'to', 'understand', 'and', 'interpret', 'human', 'language', '.', 'it', 'involves', 'tasks', 'such', 'as', 'text', 'classification', ',', 'sentiment', 'analysis', ',', 'and', 'language', 'generation', '.', 'nlp', 'has', 'revolutionized', 'the', 'way', 'we', 'interact', 'with', 'technology', ',', 'powering', 'voice', 'assistants', 'and', 'chatbots', 'that', 'can', 'understand', 'and', 'respond', 'to', 'human', 'queries', '.', 'in', 'addition', 'to', 'nlp', ',', 'ai', 'has', 'also', 'made', 'significant', 'contributions', 'to', 'computer', 'vision', '.', 'computer', 'vision', 'algorithms', 'can', 'analyze', 'and', 'interpret', 'visual', 'data', ',', 'enabling', 'machines', 'to', 'recognize', 'objects', ',', 'faces', ',', 'and', 'even', 'emotions', '.', 'this', 'technology', 'has', 'found', 'applications', 'in', 'autonomous', 'vehicles', ',', 'surveillance', 'systems', ',', 'and', 'medical', 'imaging', '.', 'the', 'ethical', 'implications', 'of', 'ai', 'have', 'also', 'garnered', 'attention', '.', 'issues', 'such', 'as', 'privacy', ',', 'bias', ',', 'and', 'accountability', 'need', 'to', 'be', 'addressed', 'to', 'ensure', 'responsible', 'ai', 'development', 'and', 'deployment', '.', 'organizations', 'and', 'policymakers', 'are', 'working', 'towards', 'establishing', 'guidelines', 'and', 'frameworks', 'to', 'govern', 'ai', 'systems', 'and', 'their', 'impact', 'on', 'society', '.', 'as', 'ai', 'continues', 'to', 'evolve', ',', 'the', 'possibilities', 'are', 'endless', '.', 'from', 'personalized', 'recommendation', 'systems', 'to', 'predictive', 'analytics', ',', 'ai', 'has', 'the', 'potential', 'to', 'transform', 'industries', 'and', 'enhance', 'human', 'capabilities', '.', 'however', ',', 'it', 'is', 'crucial', 'to', 'strike', 'a', 'balance', 'between', 'technological', 'advancements', 'and', 'ethical', 'considerations', 'to', 'harness', 'the', 'full', 'potential', 'of', 'ai', 'for', 'the', 'benefit', 'of', 'humanity', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenized Words: \", len(tokenized_word) , tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ded9c1",
   "metadata": {},
   "source": [
    "## Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17ccb2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'themselves', 'for', 'between', 'is', 'which', 'more', 'to', \"you'd\", 'under', 'both', 'needn', 'hadn', 'than', 'what', 'again', 'by', 'who', \"isn't\", 'at', 'having', 'been', 'those', 'same', \"you'll\", 'about', 'why', 'once', \"mightn't\", \"haven't\", 'am', 'him', 'whom', \"needn't\", \"won't\", 's', 'shan', \"aren't\", 'but', 'her', 'that', 'where', 'wouldn', 'of', \"shouldn't\", 'has', 'my', 'don', 'them', 'isn', 'other', 'into', 'had', \"wouldn't\", 'yours', 'on', 'the', 'o', 've', 'yourself', 'further', 'own', 'your', 'won', 'they', \"it's\", 'then', 'will', 'through', 'out', 'ours', 'herself', 'doing', 'when', 'if', \"that'll\", 'you', 'off', 'didn', 'himself', 'should', 'll', 'after', 'how', 'ain', 'm', 'she', 'just', \"should've\", 'aren', 'it', \"hadn't\", 'we', 'during', 'theirs', 'are', 'such', \"mustn't\", 'these', 'down', 'shouldn', 'over', 'there', 'was', 'his', \"couldn't\", 'me', 'he', \"weren't\", 'do', 'itself', 'does', 'hers', 'couldn', 'not', 'and', 'mightn', 'its', 'while', 'few', 'a', 'wasn', 'our', 'ourselves', 'until', 'were', \"hasn't\", 'against', \"don't\", \"didn't\", \"doesn't\", 'did', 'all', 'any', 'no', 'yourselves', 're', 'an', 'y', 'so', 'myself', 'can', 'haven', 'their', 'because', \"you've\", 'in', \"she's\", 'here', 'mustn', 'i', \"shan't\", 'doesn', 'd', 'this', \"wasn't\", 'have', \"you're\", 'only', 'very', 'hasn', 'above', 'with', 'from', 'nor', 'each', 'be', 'as', 'before', 'some', 'too', 'weren', 'or', 'most', 't', 'now', 'being', 'up', 'ma', 'below'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "529af796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Text: \n",
      "['artificial', 'intelligence', ',', 'ai', ',', 'become', 'significant', 'field', 'study', 'research', 'recent', 'years', '.', 'ability', 'simulate', 'human', 'intelligence', 'perform', 'complex', 'tasks', ',', 'ai', 'found', 'applications', 'various', 'domains', 'healthcare', ',', 'finance', ',', 'technology', '.', 'advancements', 'machine', 'learning', 'deep', 'learning', 'algorithms', 'enabled', 'ai', 'systems', 'analyze', 'vast', 'amounts', 'data', 'make', 'informed', 'decisions', '.', 'one', 'key', 'areas', 'ai', 'made', 'significant', 'strides', 'natural', 'language', 'processing', ',', 'nlp', '.', 'nlp', 'focuses', 'enabling', 'computers', 'understand', 'interpret', 'human', 'language', '.', 'involves', 'tasks', 'text', 'classification', ',', 'sentiment', 'analysis', ',', 'language', 'generation', '.', 'nlp', 'revolutionized', 'way', 'interact', 'technology', ',', 'powering', 'voice', 'assistants', 'chatbots', 'understand', 'respond', 'human', 'queries', '.', 'addition', 'nlp', ',', 'ai', 'also', 'made', 'significant', 'contributions', 'computer', 'vision', '.', 'computer', 'vision', 'algorithms', 'analyze', 'interpret', 'visual', 'data', ',', 'enabling', 'machines', 'recognize', 'objects', ',', 'faces', ',', 'even', 'emotions', '.', 'technology', 'found', 'applications', 'autonomous', 'vehicles', ',', 'surveillance', 'systems', ',', 'medical', 'imaging', '.', 'ethical', 'implications', 'ai', 'also', 'garnered', 'attention', '.', 'issues', 'privacy', ',', 'bias', ',', 'accountability', 'need', 'addressed', 'ensure', 'responsible', 'ai', 'development', 'deployment', '.', 'organizations', 'policymakers', 'working', 'towards', 'establishing', 'guidelines', 'frameworks', 'govern', 'ai', 'systems', 'impact', 'society', '.', 'ai', 'continues', 'evolve', ',', 'possibilities', 'endless', '.', 'personalized', 'recommendation', 'systems', 'predictive', 'analytics', ',', 'ai', 'potential', 'transform', 'industries', 'enhance', 'human', 'capabilities', '.', 'however', ',', 'crucial', 'strike', 'balance', 'technological', 'advancements', 'ethical', 'considerations', 'harness', 'full', 'potential', 'ai', 'benefit', 'humanity', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_sent = [w for w in tokenized_word if w not in stop]\n",
    "\n",
    "print('Filtered Text: ')\n",
    "print(filtered_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd290922",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10aa60cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40809a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artificial  --->  ADJ\n",
      "intelligence  --->  NOUN\n",
      ",  --->  .\n",
      "ai  --->  NOUN\n",
      ",  --->  .\n",
      "become  --->  VERB\n",
      "significant  --->  ADJ\n",
      "field  --->  NOUN\n",
      "study  --->  NOUN\n",
      "research  --->  NOUN\n",
      "recent  --->  ADJ\n",
      "years  --->  NOUN\n",
      ".  --->  .\n",
      "ability  --->  NOUN\n",
      "simulate  --->  NOUN\n",
      "human  --->  ADJ\n",
      "intelligence  --->  NOUN\n",
      "perform  --->  NOUN\n",
      "complex  --->  ADJ\n",
      "tasks  --->  NOUN\n",
      ",  --->  .\n",
      "ai  --->  VERB\n",
      "found  --->  VERB\n",
      "applications  --->  NOUN\n",
      "various  --->  ADJ\n",
      "domains  --->  NOUN\n",
      "healthcare  --->  NOUN\n",
      ",  --->  .\n",
      "finance  --->  NOUN\n",
      ",  --->  .\n",
      "technology  --->  NOUN\n",
      ".  --->  .\n",
      "advancements  --->  NOUN\n",
      "machine  --->  NOUN\n",
      "learning  --->  VERB\n",
      "deep  --->  ADJ\n",
      "learning  --->  NOUN\n",
      "algorithms  --->  NOUN\n",
      "enabled  --->  VERB\n",
      "ai  --->  ADJ\n",
      "systems  --->  NOUN\n",
      "analyze  --->  VERB\n",
      "vast  --->  ADJ\n",
      "amounts  --->  NOUN\n",
      "data  --->  NOUN\n",
      "make  --->  VERB\n",
      "informed  --->  ADJ\n",
      "decisions  --->  NOUN\n",
      ".  --->  .\n",
      "one  --->  NUM\n",
      "key  --->  ADJ\n",
      "areas  --->  NOUN\n",
      "ai  --->  VERB\n",
      "made  --->  VERB\n",
      "significant  --->  ADJ\n",
      "strides  --->  NOUN\n",
      "natural  --->  ADJ\n",
      "language  --->  NOUN\n",
      "processing  --->  NOUN\n",
      ",  --->  .\n",
      "nlp  --->  ADV\n",
      ".  --->  .\n",
      "nlp  --->  ADJ\n",
      "focuses  --->  NOUN\n",
      "enabling  --->  VERB\n",
      "computers  --->  NOUN\n",
      "understand  --->  VERB\n",
      "interpret  --->  ADJ\n",
      "human  --->  ADJ\n",
      "language  --->  NOUN\n",
      ".  --->  .\n",
      "involves  --->  VERB\n",
      "tasks  --->  NOUN\n",
      "text  --->  ADJ\n",
      "classification  --->  NOUN\n",
      ",  --->  .\n",
      "sentiment  --->  NOUN\n",
      "analysis  --->  NOUN\n",
      ",  --->  .\n",
      "language  --->  NOUN\n",
      "generation  --->  NOUN\n",
      ".  --->  .\n",
      "nlp  --->  CONJ\n",
      "revolutionized  --->  ADJ\n",
      "way  --->  NOUN\n",
      "interact  --->  ADJ\n",
      "technology  --->  NOUN\n",
      ",  --->  .\n",
      "powering  --->  VERB\n",
      "voice  --->  NOUN\n",
      "assistants  --->  NOUN\n",
      "chatbots  --->  NOUN\n",
      "understand  --->  VERB\n",
      "respond  --->  NOUN\n",
      "human  --->  ADJ\n",
      "queries  --->  NOUN\n",
      ".  --->  .\n",
      "addition  --->  NOUN\n",
      "nlp  --->  NOUN\n",
      ",  --->  .\n",
      "ai  --->  NOUN\n",
      "also  --->  ADV\n",
      "made  --->  VERB\n",
      "significant  --->  ADJ\n",
      "contributions  --->  NOUN\n",
      "computer  --->  NOUN\n",
      "vision  --->  NOUN\n",
      ".  --->  .\n",
      "computer  --->  NOUN\n",
      "vision  --->  NOUN\n",
      "algorithms  --->  ADP\n",
      "analyze  --->  NOUN\n",
      "interpret  --->  ADJ\n",
      "visual  --->  ADJ\n",
      "data  --->  NOUN\n",
      ",  --->  .\n",
      "enabling  --->  VERB\n",
      "machines  --->  NOUN\n",
      "recognize  --->  ADJ\n",
      "objects  --->  NOUN\n",
      ",  --->  .\n",
      "faces  --->  VERB\n",
      ",  --->  .\n",
      "even  --->  ADV\n",
      "emotions  --->  NOUN\n",
      ".  --->  .\n",
      "technology  --->  NOUN\n",
      "found  --->  VERB\n",
      "applications  --->  NOUN\n",
      "autonomous  --->  ADJ\n",
      "vehicles  --->  NOUN\n",
      ",  --->  .\n",
      "surveillance  --->  NOUN\n",
      "systems  --->  NOUN\n",
      ",  --->  .\n",
      "medical  --->  ADJ\n",
      "imaging  --->  NOUN\n",
      ".  --->  .\n",
      "ethical  --->  ADJ\n",
      "implications  --->  NOUN\n",
      "ai  --->  VERB\n",
      "also  --->  ADV\n",
      "garnered  --->  VERB\n",
      "attention  --->  NOUN\n",
      ".  --->  .\n",
      "issues  --->  NOUN\n",
      "privacy  --->  NOUN\n",
      ",  --->  .\n",
      "bias  --->  NOUN\n",
      ",  --->  .\n",
      "accountability  --->  NOUN\n",
      "need  --->  VERB\n",
      "addressed  --->  VERB\n",
      "ensure  --->  VERB\n",
      "responsible  --->  ADJ\n",
      "ai  --->  NOUN\n",
      "development  --->  NOUN\n",
      "deployment  --->  NOUN\n",
      ".  --->  .\n",
      "organizations  --->  NOUN\n",
      "policymakers  --->  NOUN\n",
      "working  --->  VERB\n",
      "towards  --->  NOUN\n",
      "establishing  --->  VERB\n",
      "guidelines  --->  NOUN\n",
      "frameworks  --->  NOUN\n",
      "govern  --->  ADJ\n",
      "ai  --->  ADJ\n",
      "systems  --->  NOUN\n",
      "impact  --->  ADJ\n",
      "society  --->  NOUN\n",
      ".  --->  .\n",
      "ai  --->  NOUN\n",
      "continues  --->  VERB\n",
      "evolve  --->  VERB\n",
      ",  --->  .\n",
      "possibilities  --->  NOUN\n",
      "endless  --->  VERB\n",
      ".  --->  .\n",
      "personalized  --->  ADJ\n",
      "recommendation  --->  NOUN\n",
      "systems  --->  NOUN\n",
      "predictive  --->  ADJ\n",
      "analytics  --->  NOUN\n",
      ",  --->  .\n",
      "ai  --->  VERB\n",
      "potential  --->  ADJ\n",
      "transform  --->  NOUN\n",
      "industries  --->  NOUN\n",
      "enhance  --->  VERB\n",
      "human  --->  ADJ\n",
      "capabilities  --->  NOUN\n",
      ".  --->  .\n",
      "however  --->  ADV\n",
      ",  --->  .\n",
      "crucial  --->  ADJ\n",
      "strike  --->  NOUN\n",
      "balance  --->  NOUN\n",
      "technological  --->  ADJ\n",
      "advancements  --->  NOUN\n",
      "ethical  --->  ADJ\n",
      "considerations  --->  NOUN\n",
      "harness  --->  VERB\n",
      "full  --->  ADJ\n",
      "potential  --->  NOUN\n",
      "ai  --->  NOUN\n",
      "benefit  --->  NOUN\n",
      "humanity  --->  NOUN\n",
      ".  --->  .\n"
     ]
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(filtered_sent, tagset='universal')\n",
    "for word, pos in tagged:\n",
    "    print(word, ' ---> ', pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d23aa3",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86a9d4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bf24186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artificial --> artifici\n",
      "intelligence --> intellig\n",
      ", --> ,\n",
      "ai --> ai\n",
      "become --> becom\n",
      "significant --> signific\n",
      "field --> field\n",
      "study --> studi\n",
      "research --> research\n",
      "recent --> recent\n",
      "years --> year\n",
      ". --> .\n",
      "ability --> abil\n",
      "simulate --> simul\n",
      "human --> human\n",
      "perform --> perform\n",
      "complex --> complex\n",
      "tasks --> task\n",
      "found --> found\n",
      "applications --> applic\n",
      "various --> variou\n",
      "domains --> domain\n",
      "healthcare --> healthcar\n",
      "finance --> financ\n",
      "technology --> technolog\n",
      "advancements --> advanc\n",
      "machine --> machin\n",
      "learning --> learn\n",
      "deep --> deep\n",
      "algorithms --> algorithm\n",
      "enabled --> enabl\n",
      "systems --> system\n",
      "analyze --> analyz\n",
      "vast --> vast\n",
      "amounts --> amount\n",
      "data --> data\n",
      "make --> make\n",
      "informed --> inform\n",
      "decisions --> decis\n",
      "one --> one\n",
      "key --> key\n",
      "areas --> area\n",
      "made --> made\n",
      "strides --> stride\n",
      "natural --> natur\n",
      "language --> languag\n",
      "processing --> process\n",
      "nlp --> nlp\n",
      "focuses --> focus\n",
      "enabling --> enabl\n",
      "computers --> comput\n",
      "understand --> understand\n",
      "interpret --> interpret\n",
      "involves --> involv\n",
      "text --> text\n",
      "classification --> classif\n",
      "sentiment --> sentiment\n",
      "analysis --> analysi\n",
      "generation --> gener\n",
      "revolutionized --> revolution\n",
      "way --> way\n",
      "interact --> interact\n",
      "powering --> power\n",
      "voice --> voic\n",
      "assistants --> assist\n",
      "chatbots --> chatbot\n",
      "respond --> respond\n",
      "queries --> queri\n",
      "addition --> addit\n",
      "also --> also\n",
      "contributions --> contribut\n",
      "computer --> comput\n",
      "vision --> vision\n",
      "visual --> visual\n",
      "machines --> machin\n",
      "recognize --> recogn\n",
      "objects --> object\n",
      "faces --> face\n",
      "even --> even\n",
      "emotions --> emot\n",
      "autonomous --> autonom\n",
      "vehicles --> vehicl\n",
      "surveillance --> surveil\n",
      "medical --> medic\n",
      "imaging --> imag\n",
      "ethical --> ethic\n",
      "implications --> implic\n",
      "garnered --> garner\n",
      "attention --> attent\n",
      "issues --> issu\n",
      "privacy --> privaci\n",
      "bias --> bia\n",
      "accountability --> account\n",
      "need --> need\n",
      "addressed --> address\n",
      "ensure --> ensur\n",
      "responsible --> respons\n",
      "development --> develop\n",
      "deployment --> deploy\n",
      "organizations --> organ\n",
      "policymakers --> policymak\n",
      "working --> work\n",
      "towards --> toward\n",
      "establishing --> establish\n",
      "guidelines --> guidelin\n",
      "frameworks --> framework\n",
      "govern --> govern\n",
      "impact --> impact\n",
      "society --> societi\n",
      "continues --> continu\n",
      "evolve --> evolv\n",
      "possibilities --> possibl\n",
      "endless --> endless\n",
      "personalized --> person\n",
      "recommendation --> recommend\n",
      "predictive --> predict\n",
      "analytics --> analyt\n",
      "potential --> potenti\n",
      "transform --> transform\n",
      "industries --> industri\n",
      "enhance --> enhanc\n",
      "capabilities --> capabl\n",
      "however --> howev\n",
      "crucial --> crucial\n",
      "strike --> strike\n",
      "balance --> balanc\n",
      "technological --> technolog\n",
      "considerations --> consider\n",
      "harness --> har\n",
      "full --> full\n",
      "benefit --> benefit\n",
      "humanity --> human\n"
     ]
    }
   ],
   "source": [
    "stemmed = {w : ps.stem(w) for w in filtered_sent}\n",
    "for word, sword in stemmed.items():\n",
    "    print(word, '-->' , sword)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f215ca5",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3badbd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a1d452d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artificial --> artificial\n",
      "intelligence --> intelligence\n",
      ", --> ,\n",
      "ai --> ai\n",
      "become --> become\n",
      "significant --> significant\n",
      "field --> field\n",
      "study --> study\n",
      "research --> research\n",
      "recent --> recent\n",
      "years --> year\n",
      ". --> .\n",
      "ability --> ability\n",
      "simulate --> simulate\n",
      "human --> human\n",
      "perform --> perform\n",
      "complex --> complex\n",
      "tasks --> task\n",
      "found --> found\n",
      "applications --> application\n",
      "various --> various\n",
      "domains --> domain\n",
      "healthcare --> healthcare\n",
      "finance --> finance\n",
      "technology --> technology\n",
      "advancements --> advancement\n",
      "machine --> machine\n",
      "learning --> learning\n",
      "deep --> deep\n",
      "algorithms --> algorithm\n",
      "enabled --> enabled\n",
      "systems --> system\n",
      "analyze --> analyze\n",
      "vast --> vast\n",
      "amounts --> amount\n",
      "data --> data\n",
      "make --> make\n",
      "informed --> informed\n",
      "decisions --> decision\n",
      "one --> one\n",
      "key --> key\n",
      "areas --> area\n",
      "made --> made\n",
      "strides --> stride\n",
      "natural --> natural\n",
      "language --> language\n",
      "processing --> processing\n",
      "nlp --> nlp\n",
      "focuses --> focus\n",
      "enabling --> enabling\n",
      "computers --> computer\n",
      "understand --> understand\n",
      "interpret --> interpret\n",
      "involves --> involves\n",
      "text --> text\n",
      "classification --> classification\n",
      "sentiment --> sentiment\n",
      "analysis --> analysis\n",
      "generation --> generation\n",
      "revolutionized --> revolutionized\n",
      "way --> way\n",
      "interact --> interact\n",
      "powering --> powering\n",
      "voice --> voice\n",
      "assistants --> assistant\n",
      "chatbots --> chatbots\n",
      "respond --> respond\n",
      "queries --> query\n",
      "addition --> addition\n",
      "also --> also\n",
      "contributions --> contribution\n",
      "computer --> computer\n",
      "vision --> vision\n",
      "visual --> visual\n",
      "machines --> machine\n",
      "recognize --> recognize\n",
      "objects --> object\n",
      "faces --> face\n",
      "even --> even\n",
      "emotions --> emotion\n",
      "autonomous --> autonomous\n",
      "vehicles --> vehicle\n",
      "surveillance --> surveillance\n",
      "medical --> medical\n",
      "imaging --> imaging\n",
      "ethical --> ethical\n",
      "implications --> implication\n",
      "garnered --> garnered\n",
      "attention --> attention\n",
      "issues --> issue\n",
      "privacy --> privacy\n",
      "bias --> bias\n",
      "accountability --> accountability\n",
      "need --> need\n",
      "addressed --> addressed\n",
      "ensure --> ensure\n",
      "responsible --> responsible\n",
      "development --> development\n",
      "deployment --> deployment\n",
      "organizations --> organization\n",
      "policymakers --> policymakers\n",
      "working --> working\n",
      "towards --> towards\n",
      "establishing --> establishing\n",
      "guidelines --> guideline\n",
      "frameworks --> framework\n",
      "govern --> govern\n",
      "impact --> impact\n",
      "society --> society\n",
      "continues --> continues\n",
      "evolve --> evolve\n",
      "possibilities --> possibility\n",
      "endless --> endless\n",
      "personalized --> personalized\n",
      "recommendation --> recommendation\n",
      "predictive --> predictive\n",
      "analytics --> analytics\n",
      "potential --> potential\n",
      "transform --> transform\n",
      "industries --> industry\n",
      "enhance --> enhance\n",
      "capabilities --> capability\n",
      "however --> however\n",
      "crucial --> crucial\n",
      "strike --> strike\n",
      "balance --> balance\n",
      "technological --> technological\n",
      "considerations --> consideration\n",
      "harness --> harness\n",
      "full --> full\n",
      "benefit --> benefit\n",
      "humanity --> humanity\n"
     ]
    }
   ],
   "source": [
    "lemmatized = {w : wnl.lemmatize(w) for w in filtered_sent}\n",
    "for word, sword in lemmatized.items():\n",
    "    print(word, '-->' , sword)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b2ed6",
   "metadata": {},
   "source": [
    "## Creating sentences of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d49cf246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Sentence:  ['Data', 'Science', 'is', 'the', 'best', 'job', 'of', 'the', '21st', 'century.']\n",
      "Second Sentence:  ['machine', 'learning', 'is', 'the', 'key', 'for', 'data', 'science']\n"
     ]
    }
   ],
   "source": [
    "first_sentence = 'Data Science is the best job of the 21st century.'\n",
    "second_sentence = 'machine learning is the key for data science'\n",
    "\n",
    "# Creating lists of sentences\n",
    "first_sent = first_sentence.split(' ')\n",
    "second_sent = second_sentence.split(' ')\n",
    "\n",
    "print('First Sentence: ', first_sent)\n",
    "print('Second Sentence: ', second_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ccc03afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentence {'21st', 'science', 'data', 'key', 'learning', 'machine', 'Data', 'century.', 'Science', 'is', 'for', 'best', 'job', 'the', 'of'}\n"
     ]
    }
   ],
   "source": [
    "# Creating total list\n",
    "\n",
    "total = set(first_sent).union(set(second_sent))\n",
    "print(\"Total Sentence\", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "440b73b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>21st</th>\n",
       "      <th>science</th>\n",
       "      <th>data</th>\n",
       "      <th>key</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>Data</th>\n",
       "      <th>century.</th>\n",
       "      <th>Science</th>\n",
       "      <th>is</th>\n",
       "      <th>for</th>\n",
       "      <th>best</th>\n",
       "      <th>job</th>\n",
       "      <th>the</th>\n",
       "      <th>of</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   21st  science  data  key  learning  machine  Data  century.  Science  is   \n",
       "0     1        0     0    0         0        0     1         1        1   1  \\\n",
       "1     0        1     1    1         1        1     0         0        0   1   \n",
       "\n",
       "   for  best  job  the  of  \n",
       "0    0     1    1    2   1  \n",
       "1    1     0    0    1   0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating dictionaries of sentence frequency\n",
    "\n",
    "wordDictA = dict.fromkeys(total, 0)\n",
    "wordDictB = dict.fromkeys(total, 0)\n",
    "\n",
    "for w in first_sent:\n",
    "    wordDictA[w] += 1\n",
    "    \n",
    "for w in second_sent:\n",
    "    wordDictB[w] += 1\n",
    "    \n",
    "pd.DataFrame([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8eb78b",
   "metadata": {},
   "source": [
    "## TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c65395a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF(wordDict, sent):\n",
    "    tf = {}\n",
    "    N = len(sent)\n",
    "    \n",
    "    for w in wordDict.keys():\n",
    "        tf[w] = wordDict[w] / float(N)\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "483130bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>21st</th>\n",
       "      <th>science</th>\n",
       "      <th>data</th>\n",
       "      <th>key</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>Data</th>\n",
       "      <th>century.</th>\n",
       "      <th>Science</th>\n",
       "      <th>is</th>\n",
       "      <th>for</th>\n",
       "      <th>best</th>\n",
       "      <th>job</th>\n",
       "      <th>the</th>\n",
       "      <th>of</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   21st  science   data    key  learning  machine  Data  century.  Science   \n",
       "0   0.1    0.000  0.000  0.000     0.000    0.000   0.1       0.1      0.1  \\\n",
       "1   0.0    0.125  0.125  0.125     0.125    0.125   0.0       0.0      0.0   \n",
       "\n",
       "      is    for  best  job    the   of  \n",
       "0  0.100  0.000   0.1  0.1  0.200  0.1  \n",
       "1  0.125  0.125   0.0  0.0  0.125  0.0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfA = TF(wordDictA, first_sent)\n",
    "tfB = TF(wordDictB, second_sent)\n",
    "\n",
    "pd.DataFrame([tfA, tfB])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a848b7a7",
   "metadata": {},
   "source": [
    "## IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b8e05bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def IDF(rowList):\n",
    "    idf = {}\n",
    "    N = len(rowList)\n",
    "    \n",
    "    idf = dict.fromkeys(rowList[0].keys(), 0)\n",
    "    \n",
    "    for word, cnt in idf.items():\n",
    "        for doc in rowList:\n",
    "            if doc[word] != 0:\n",
    "                idf[word] += 1\n",
    "    \n",
    "    for word, cnt in idf.items():\n",
    "        idf[word] = math.log10(N / (float(cnt)))\n",
    "    \n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "733a8d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'21st': 0.3010299956639812, 'science': 0.3010299956639812, 'data': 0.3010299956639812, 'key': 0.3010299956639812, 'learning': 0.3010299956639812, 'machine': 0.3010299956639812, 'Data': 0.3010299956639812, 'century.': 0.3010299956639812, 'Science': 0.3010299956639812, 'is': 0.0, 'for': 0.3010299956639812, 'best': 0.3010299956639812, 'job': 0.3010299956639812, 'the': 0.0, 'of': 0.3010299956639812}\n"
     ]
    }
   ],
   "source": [
    "idf = IDF([wordDictA, wordDictB])\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1a58b406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF1(wordDic, sent):\n",
    "    tf = {}\n",
    "    N = len(sent)\n",
    "    \n",
    "    for w, c in wordDic.items():\n",
    "        tf[w] = c / float(N)\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "92ffc64d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>21st</th>\n",
       "      <th>science</th>\n",
       "      <th>data</th>\n",
       "      <th>key</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>Data</th>\n",
       "      <th>century.</th>\n",
       "      <th>Science</th>\n",
       "      <th>is</th>\n",
       "      <th>for</th>\n",
       "      <th>best</th>\n",
       "      <th>job</th>\n",
       "      <th>the</th>\n",
       "      <th>of</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   21st  science   data    key  learning  machine  Data  century.  Science   \n",
       "0   0.1    0.000  0.000  0.000     0.000    0.000   0.1       0.1      0.1  \\\n",
       "1   0.0    0.125  0.125  0.125     0.125    0.125   0.0       0.0      0.0   \n",
       "\n",
       "      is    for  best  job    the   of  \n",
       "0  0.100  0.000   0.1  0.1  0.200  0.1  \n",
       "1  0.125  0.125   0.0  0.0  0.125  0.0  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfA1 = TF1(wordDictA, first_sent)\n",
    "tfB1 = TF1(wordDictB, second_sent)\n",
    "\n",
    "pd.DataFrame([tfA1, tfB1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b3b769fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IDF1(docList):\n",
    "    idf = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idf = dict.fromkeys(docList[0].keys(), 0)\n",
    "    \n",
    "    for w in idf.keys():\n",
    "        cnt = 0\n",
    "        for doc in docList:\n",
    "            if doc[w] > 0:\n",
    "                cnt += 1\n",
    "            idf[w] = cnt\n",
    "            \n",
    "    for w, c in idf.items():\n",
    "        idf[w] = math.log10(N/floor(c))\n",
    "    \n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a35801dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'21st': 0.3010299956639812, 'science': 0.3010299956639812, 'data': 0.3010299956639812, 'key': 0.3010299956639812, 'learning': 0.3010299956639812, 'machine': 0.3010299956639812, 'Data': 0.3010299956639812, 'century.': 0.3010299956639812, 'Science': 0.3010299956639812, 'is': 0.0, 'for': 0.3010299956639812, 'best': 0.3010299956639812, 'job': 0.3010299956639812, 'the': 0.0, 'of': 0.3010299956639812}\n"
     ]
    }
   ],
   "source": [
    "idf1 = IDF1([wordDictA, wordDictB])\n",
    "print(idf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ec5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
